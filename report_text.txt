The physical situation simulated for this project was electrostatics, with fixed potentials. The only equation, thus, that must be solved is Laplace's equation:
	laplace
This can then be simplified further, as for all points for which the equation must be solved, the charge density is zero. The boundary conditions for the simulation were Dirichlet as the value of the potential was fixed.



In order to verify the accuracy of the method, a simple test case was considered. A single point in the centre of the mesh was given a positive potential, while the borders of the mesh were locked at zero. The mesh was then made large to allow for the outer boundaries to be approximated as infinity. The solution to Laplace's equation in such a scenario is a 1/r relation with distance from the source. As Fig. () and () show, the computed solution is very close to the analytic solution.

The next test case chosen was that of an infinite square coaxial cable. This was a similar example to the first case, however the area of positive potential was now a ninth of the total mesh. The solution of this was non-analytic, however it was a useful case to further refine the algorithm. At large mesh sizes for the coaxial cable, the time for the algorithm to converge became similarly large and thus optimisations were considered. The most powerful of these was a shift from single core, serial calculation, to a multi core, parallel approach. In order to implement this, the POSIX threading library was used for the C code. In order to avoid any conflicts whereby the variable being accessed is changed by another thread, the red-black successive over-relaxation method was used[cite]. This involved dividing the mesh into red and black squares similar to a chess board (see Fig. ()). This mesh then has the property that all red squares have only black neighbours and vice versa. In turn this allows for an arbitrary level of parallelisation as all calculations are now independent of each other. 

Implementing this method was relatively simple as the potential mesh was located in shared memory for all the cores. This meant that each thread updated the same array and no joining of separate arrays was required. The original array was divided by the number of cores available and each core worked on an independent part of the mesh. No communication between cores was required as each square was independent from the rest. The scalability of the code was checked on a 48 core server cluster, however long calculations could not be achieved on such a machine due to limited access.

In theory, the red-black method should increase performance by a factor of the number of cores used. This was not, however, the case for two primary reasons. The first of these was the overhead of creating threads, which involved creating an InitParams struct for the function arguments for each core. Furthermore, all threads had to finish before the other colour could be calculated, adding a small wait time. The other reason was load management in the CPU itself. Having all cores running at 100% load would cause the CPU to rapidly overheat and potentially damage the computer. For this reason, the load was generally limited to approximately 80% (this still caused the core temperatures to be approximately 80°C for long runs). The effect of these two factors can be seen in Fig. ().

 
